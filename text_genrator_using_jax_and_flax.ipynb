{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmP1BvMdCHSaM34anplN9R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Utkarsh-Aggarwal/local-repo/blob/main/text_genrator_using_jax_and_flax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7c-FnmUCBC9i",
        "outputId": "01d7521f-d6be-4a09-a727-7bc217eebaa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (0.4.33)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.11/dist-packages (0.10.3)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (0.2.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax) (1.13.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax) (1.1.0)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax) (0.6.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax) (0.1.72)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.11/dist-packages (from flax) (13.9.4)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from flax) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from flax) (0.1.9)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.11/dist-packages (from optax) (0.1.89)\n",
            "Requirement already satisfied: etils[epy] in /usr/local/lib/python3.11/dist-packages (from optax) (1.12.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.87->optax) (0.12.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax) (2.18.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax) (4.25.6)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax) (4.11.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epy]->optax) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epy]->optax) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epy]->optax) (3.21.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install jax jaxlib flax optax numpy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from flax import linen as nn\n",
        "import optax\n",
        "import jax.random as random\n",
        "from functools import partial\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"hello world! hello jax! hello flax! \"\n",
        "\n",
        "# Create a vocabulary (unique characters) and mappings.\n",
        "vocab = sorted(list(set(corpus)))\n",
        "vocab_size = len(vocab)\n",
        "char2idx = {ch: i for i, ch in enumerate(vocab)}\n",
        "idx2char = {i: ch for i, ch in enumerate(vocab)}\n",
        "\n",
        "# Convert the entire corpus into a sequence of integer indices.\n",
        "data = np.array([char2idx[c] for c in corpus], dtype=np.int32)"
      ],
      "metadata": {
        "id": "PQ08AnqFbcF8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 32        # Embedding dimension for tokens.\n",
        "num_heads = 2         # Number of attention heads.\n",
        "num_layers = 2        # Number of stacked transformer blocks.\n",
        "ff_dim = 64           # Hidden dimension in the feed-forward network.\n",
        "block_size = 16       # Maximum sequence length (context window).\n",
        "dropout_rate = 0.1    # Dropout rate.\n",
        "learning_rate = 1e-3  # Learning rate for optimizer.\n",
        "num_epochs = 1000     # Total training epochs.\n",
        "batch_size = 16       # Training batch size.\n"
      ],
      "metadata": {
        "id": "PgW6zIPsbjLL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(seq_len, d_model):\n",
        "    \"\"\"\n",
        "    Computes a fixed sinusoidal positional encoding.\n",
        "\n",
        "    Args:\n",
        "      seq_len: Length of the sequence.\n",
        "      d_model: Dimension of the embeddings.\n",
        "\n",
        "    Returns:\n",
        "      A JAX array of shape (seq_len, d_model) containing the positional encodings.\n",
        "    \"\"\"\n",
        "    pos = np.arange(seq_len)[:, np.newaxis]  # Shape (seq_len, 1)\n",
        "    i = np.arange(d_model)[np.newaxis, :]      # Shape (1, d_model)\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    pos_encoding = pos * angle_rates\n",
        "    # Apply sin to even indices and cos to odd indices.\n",
        "    pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n",
        "    pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n",
        "    return jnp.array(pos_encoding)"
      ],
      "metadata": {
        "id": "rC4iYTQQbnH6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"A single transformer block consisting of self-attention and feed-forward layers.\"\"\"\n",
        "    embed_dim: int\n",
        "    num_heads: int\n",
        "    ff_dim: int\n",
        "    dropout_rate: float\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, deterministic=True):\n",
        "        # Multi-head self-attention sub-layer\n",
        "        residual = x\n",
        "        x = nn.LayerNorm()(x)\n",
        "        x = nn.SelfAttention(\n",
        "            num_heads=self.num_heads,\n",
        "            qkv_features=self.embed_dim,\n",
        "            dropout_rate=self.dropout_rate,\n",
        "            deterministic=deterministic\n",
        "        )(x)\n",
        "        x = x + residual  # Residual connection\n",
        "\n",
        "        # Feed-forward network sub-layer\n",
        "        residual = x\n",
        "        x = nn.LayerNorm()(x)\n",
        "        x = nn.Dense(self.ff_dim)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)\n",
        "        x = nn.Dense(self.embed_dim)(x)\n",
        "        x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)\n",
        "        x = x + residual  # Residual connection\n",
        "        return x"
      ],
      "metadata": {
        "id": "jmwFeKw-b0fF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TransformerLM(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer-based language model.\n",
        "\n",
        "    Args:\n",
        "      vocab_size: Number of tokens in the vocabulary.\n",
        "      embed_dim: Dimension of token embeddings.\n",
        "      num_heads: Number of attention heads.\n",
        "      num_layers: Number of transformer blocks.\n",
        "      ff_dim: Dimension of the feed-forward network.\n",
        "      block_size: Maximum length of input sequence.\n",
        "      dropout_rate: Dropout rate.\n",
        "    \"\"\"\n",
        "    vocab_size: int\n",
        "    embed_dim: int\n",
        "    num_heads: int\n",
        "    num_layers: int\n",
        "    ff_dim: int\n",
        "    block_size: int\n",
        "    dropout_rate: float\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, deterministic=True):\n",
        "        # x shape: (batch, sequence_length)\n",
        "        # Token embedding layer\n",
        "        x = nn.Embed(num_embeddings=self.vocab_size, features=self.embed_dim)(x)\n",
        "\n",
        "        # Instead of using a fixed positional encoding of shape (block_size, embed_dim),\n",
        "        # we compute positional encoding based on the actual sequence length.\n",
        "        seq_len = x.shape[1]\n",
        "        pos_enc = positional_encoding(seq_len, self.embed_dim)\n",
        "        x = x + pos_enc\n",
        "\n",
        "        # Apply a stack of transformer blocks.\n",
        "        for _ in range(self.num_layers):\n",
        "            x = TransformerBlock(\n",
        "                embed_dim=self.embed_dim,\n",
        "                num_heads=self.num_heads,\n",
        "                ff_dim=self.ff_dim,\n",
        "                dropout_rate=self.dropout_rate\n",
        "            )(x, deterministic=deterministic)\n",
        "\n",
        "        # Final layer normalization.\n",
        "        x = nn.LayerNorm()(x)\n",
        "\n",
        "        # Project the outputs to logits for each vocabulary token.\n",
        "        logits = nn.Dense(self.vocab_size)(x)\n",
        "        return logits\n",
        "\n",
        "# Instantiate the model.\n",
        "model = TransformerLM(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=embed_dim,\n",
        "    num_heads=num_heads,\n",
        "    num_layers=num_layers,\n",
        "    ff_dim=ff_dim,\n",
        "    block_size=block_size,\n",
        "    dropout_rate=dropout_rate\n",
        ")\n",
        "\n",
        "#######################################\n",
        "# INITIALIZATION\n",
        "#######################################\n",
        "# Create a random key for initialization and training.\n",
        "rng = random.PRNGKey(0)\n",
        "# Dummy input for shape inference (batch_size x block_size).\n",
        "dummy_input = jnp.ones((batch_size, block_size), dtype=jnp.int32)\n",
        "params = model.init(rng, dummy_input)"
      ],
      "metadata": {
        "id": "tklRjM6zb6oN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(logits, targets):\n",
        "    \"\"\"\n",
        "    Computes the cross-entropy loss between predicted logits and target tokens.\n",
        "\n",
        "    Args:\n",
        "      logits: Logits from the model of shape (batch, seq_length, vocab_size).\n",
        "      targets: Ground truth token indices of shape (batch, seq_length).\n",
        "\n",
        "    Returns:\n",
        "      The mean cross-entropy loss.\n",
        "    \"\"\"\n",
        "    one_hot_targets = jax.nn.one_hot(targets, logits.shape[-1])\n",
        "    loss = optax.softmax_cross_entropy(logits, one_hot_targets)\n",
        "    return loss.mean()\n",
        "\n",
        "# Set up the Adam optimizer using optax.\n",
        "optimizer = optax.adam(learning_rate)\n",
        "opt_state = optimizer.init(params)\n"
      ],
      "metadata": {
        "id": "vwH0TrmQcBht"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def train_step(params, opt_state, batch, rng):\n",
        "    \"\"\"\n",
        "    Performs one training step: computes loss, gradients, and updates parameters.\n",
        "\n",
        "    Args:\n",
        "      params: Model parameters.\n",
        "      opt_state: Optimizer state.\n",
        "      batch: Batch of token sequences (shape: (batch, block_size)).\n",
        "      rng: Random key for dropout.\n",
        "\n",
        "    Returns:\n",
        "      Updated parameters, updated optimizer state, and the loss value.\n",
        "    \"\"\"\n",
        "    def loss_fn(params):\n",
        "        # Forward pass: obtain logits for the batch.\n",
        "        logits = model.apply(params, batch, deterministic=False, rngs={'dropout': rng})\n",
        "        # Use tokens 0 to block_size-1 as input and predict tokens 1 to block_size.\n",
        "        loss = cross_entropy_loss(logits[:, :-1], batch[:, 1:])\n",
        "        return loss\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, opt_state, loss\n",
        "\n",
        "#######################################\n",
        "# TRAINING LOOP\n",
        "#######################################\n",
        "def get_batch(data, batch_size, block_size):\n",
        "    \"\"\"\n",
        "    Creates a batch of input sequences from the data.\n",
        "\n",
        "    Args:\n",
        "      data: Array of token indices.\n",
        "      batch_size: Number of sequences in a batch.\n",
        "      block_size: Length of each sequence.\n",
        "\n",
        "    Returns:\n",
        "      A JAX array of shape (batch_size, block_size).\n",
        "    \"\"\"\n",
        "    n = len(data) - block_size\n",
        "    idx = np.random.randint(0, n, (batch_size,))\n",
        "    batch = np.stack([data[i:i+block_size] for i in idx])\n",
        "    return jnp.array(batch)\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    batch = get_batch(data, batch_size, block_size)\n",
        "    rng, step_rng = random.split(rng)\n",
        "    params, opt_state, loss = train_step(params, opt_state, batch, step_rng)\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wmxYAgtncIiE",
        "outputId": "b125ab5e-0bab-42cd-82c1-9909bf10a56b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 0, Loss: 3.0474\n",
            "Epoch 100, Loss: 0.6668\n",
            "Epoch 200, Loss: 0.2297\n",
            "Epoch 300, Loss: 0.1495\n",
            "Epoch 400, Loss: 0.0968\n",
            "Epoch 500, Loss: 0.0470\n",
            "Epoch 600, Loss: 0.0805\n",
            "Epoch 700, Loss: 0.0156\n",
            "Epoch 800, Loss: 0.0281\n",
            "Epoch 900, Loss: 0.0144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(params, seed_text, length, rng):\n",
        "    \"\"\"\n",
        "    Generates text using the trained model.\n",
        "\n",
        "    Args:\n",
        "      params: Trained model parameters.\n",
        "      seed_text: Initial text to seed generation.\n",
        "      length: Number of tokens to generate.\n",
        "      rng: Random key.\n",
        "\n",
        "    Returns:\n",
        "      A string containing the generated text.\n",
        "    \"\"\"\n",
        "    # Convert the seed text to token indices.\n",
        "    input_seq = jnp.array([char2idx[c] for c in seed_text], dtype=jnp.int32)[None, :]\n",
        "    generated = list(seed_text)\n",
        "    for _ in range(length):\n",
        "        # If sequence is longer than block_size, use only the last block_size tokens.\n",
        "        input_seq_cond = input_seq[:, -block_size:]\n",
        "        logits = model.apply(params, input_seq_cond, deterministic=True)\n",
        "        # Get logits for the last token.\n",
        "        logits = logits[:, -1, :]\n",
        "        # Use greedy sampling: pick the token with the highest logit.\n",
        "        next_token = jnp.argmax(logits, axis=-1)\n",
        "        next_token = int(next_token[0])\n",
        "        generated.append(idx2char[next_token])\n",
        "        # Append the predicted token to the sequence.\n",
        "        input_seq = jnp.concatenate([input_seq, jnp.array([[next_token]], dtype=jnp.int32)], axis=1)\n",
        "    return \"\".join(generated)"
      ],
      "metadata": {
        "id": "wzGV5yFVfIyf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = \"hello\"\n",
        "generated_text = generate_text(params, seed, length=50, rng=rng)\n",
        "print(\"Generated text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WG5K0KDgfO5y",
        "outputId": "fd2caf8d-ad23-48a4-fa4e-c970ea4eebf6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "hello jax! helllo flax! hello jax! helllo flax! hello j\n"
          ]
        }
      ]
    }
  ]
}